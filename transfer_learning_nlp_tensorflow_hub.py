# -*- coding: utf-8 -*-
"""Transfer-Learning-NLP-TensorFlow-Hub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRlk62Ue1p--C9WL8JE9LNBOtBV1TRVg
"""

import numpy as np 
import pandas as pd 

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (12, 8)
from  IPython import display

import pathlib
import shutil
import tempfile

!pip install -q git+https://github.com/tensorflow/docs

import tensorflow_docs as tfdocs
import tensorflow_docs.modeling
import tensorflow_docs.plots

print("Tensorflow Version: ", tf.__version__)
print("Hub version: ", hub.__version__)

logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

df = pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip', compression = 'zip', low_memory = False)

df.shape

df.sample(3)

df['target'].plot(kind = 'hist', title = 'target distribution');

from sklearn.model_selection import train_test_split

train_df, remaining = train_test_split(df, random_state = 42, train_size = 0.01, stratify = df.target.values)
valid_df, _ = train_test_split(remaining, random_state= 42, train_size = 0.001, stratify = remaining.target.values)

train_df.shape, valid_df.shape

''' 
  Stratified Sampling : training set et validation set suivent le même distribution que le data frame
'''
# distribution de notre data frame
print(df['target'].value_counts(normalize=True), '\n\n')

# training set suit la même distribution
print(train_df['target'].value_counts(normalize=True), '\n\n')

# validation set suit la même distribution
print(valid_df.target.value_counts(normalize=True))

train_df['target'].head(20).values

train_df.question_text.head(20).values

# Définir une fonction pour construire et compiler les modèles 

def train_and_evaluate_model(module_url, embed_size, name, trainable=False):

  hub_layer = hub.KerasLayer(module_url, input_shape=[], output_shape=[embed_size], dtype=tf.string, trainable=trainable)
  
  model = tf.keras.models.Sequential([
                                      hub_layer,
                                      tf.keras.layers.Dense(256, activation='relu'),
                                      tf.keras.layers.Dense(64, activation='relu'),
                                      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.losses.BinaryCrossentropy(),
              metrics=[tf.metrics.BinaryAccuracy(name='accuracy')])
  model.summary()
  history = model.fit(train_df['question_text'], train_df['target'],
                    epochs=100,
                    batch_size=32,
                    validation_data=(valid_df['question_text'], valid_df['target']),
                    callbacks=[tfdocs.modeling.EpochDots(),
                               tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min'),
                               tf.keras.callbacks.TensorBoard(logdir/name)],
                    verbose=0)
  return history

histories = {}

# importer les models

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/5" #@param ["https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1", "https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1", "https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1", "https://tfhub.dev/google/universal-sentence-encoder/4", "https://tfhub.dev/google/universal-sentence-encoder-large/5"] {allow-input: true}

histories['universal-sentence-encoder-large'] = train_and_evaluate_model(module_url, embed_size = 512, name = 'universal-sentence-encoder-large', trainable = False)

# Comparer l'Accuracy et Loss Curves 

plt.rcParams['figure.figsize'] = (12, 8)
plotter = tfdocs.plots.HistoryPlotter(metric = 'accuracy')
plotter.plot(histories)
plt.xlabel("Epochs")
plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')
plt.title("Accuracy Curves for Models")
plt.show()

# NOTE : bigger models (plus grand nombre de trainable param) are doing better (enfin, généralement)
# NOTE_1 : Ici, on a un sur apprentissage (overfitting), il y a un gap entre Train et Val ( Val > Train), pour les trois premiers modèles

plotter = tfdocs.plots.HistoryPlotter(metric = 'loss')
plotter.plot(histories)
plt.xlabel("Epochs")
plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')
plt.title("Loss Curves for Models")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Visualiser les metriques avec TensorBoard

# %load_ext tensorboard

# %tensorboard --logdir {logdir}
